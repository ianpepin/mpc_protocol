{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import fasttext\n",
    "import sys\n",
    "import torch\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import unidecode\n",
    "from natsort import natsorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths of different files used in this script\n",
    "vectors_filename = \"/home/jovyan/embeddings/BioWordVec_PubMed_MIMICIII_d200.vec.bin\"\n",
    "model_filename = \"/home/jovyan/embeddings/BioWordVec_PubMed_MIMICIII_d200.bin\"\n",
    "\n",
    "deid_notes_path = \"/home/jovyan/mpc_use_case/three_party_mpc/party1_unstructured/data/deidentified_notes\"\n",
    "oa_patients_combined_notes = \"/home/jovyan/mpc_use_case/three_party_mpc/party1_unstructured/data/oa_patients_notes\"\n",
    "other_patients_combined_notes = \"/home/jovyan/mpc_use_case/three_party_mpc/party1_unstructured/data/other_patients_notes\"\n",
    "\n",
    "oa_patients_tensors = \"/home/jovyan/mpc_use_case/three_party_mpc/party1_unstructured/data/oa_patients_tensors\"\n",
    "oa_patients_encrypted_tensors = \"/home/jovyan/mpc_use_case/three_party_mpc/party1_unstructured/data/oa_patients_encrypted_tensors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors loaded\n"
     ]
    }
   ],
   "source": [
    "# Loads the word embeddings from the vector binary file\n",
    "bioword_vector = KeyedVectors.load_word2vec_format(vectors_filename, binary=True)\n",
    "print(\"Vectors loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# Loads the BioWordVec model, which we can use to generate embeddings for OOV words\n",
    "bioword_model = fasttext.load_model(model_filename)\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing patients and their notes based on their diagnosis\n",
      "Number of patients having patient notes: 163\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nProcessing patients and their notes based on their diagnosis\")\n",
    "\n",
    "# Fetching demographic_no for all patients from the filename of notes\n",
    "files = natsorted(os.listdir(deid_notes_path))\n",
    "all_demographic_nos_notes = set()\n",
    "# This is a list that contains the note IDs for each patient\n",
    "for file in files:\n",
    "    demographic_no = int(file.split(\"-\")[1].split(\".\")[0])\n",
    "    all_demographic_nos_notes.add(demographic_no)\n",
    "    \n",
    "list_of_files = [[] for _ in range(len(all_demographic_nos_notes))]\n",
    "for file in files:\n",
    "    demographic_no = int(file.split(\"-\")[1].split(\".\")[0])\n",
    "    note_id = int(file.split(\"-\")[0].split(\".\")[0])\n",
    "    # Add the note ID to the list of notes for the patient\n",
    "    list_of_files[demographic_no-1].append(note_id)\n",
    "print(\"Number of patients having patient notes:\", len(all_demographic_nos_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of patients listed with disease code: 163\n",
      "Number of patients listed in disease code table as having OA: 33\n",
      "Patient IDs: [4, 5, 6, 7, 8, 9, 11, 14, 18, 26, 37, 40, 54, 58, 61, 63, 64, 76, 77, 83, 94, 101, 103, 106, 110, 115, 133, 135, 148, 150, 155, 159, 162]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#! -----------------------------------------------------------------------------------------\n",
    "#TODO Structured data\n",
    "#! -----------------------------------------------------------------------------------------\n",
    "all_demographic_nos_dxresearch = set()\n",
    "oa_patients = set()\n",
    "# Convert txt to csv\n",
    "with open('/home/jovyan/mpc_use_case/structured_data/DxResearch.txt', 'r') as in_file:\n",
    "    stripped = (line.strip() for line in in_file)\n",
    "    lines = (line.split(\",\") for line in stripped if line)\n",
    "    with open('/home/jovyan/mpc_use_case/prototype/oaTypes/DxResearch.csv', 'w') as out_file:\n",
    "        writer = csv.writer(out_file)\n",
    "        writer.writerows(lines)\n",
    "\n",
    "#! -----------------------------------------------------------------------------------------\n",
    "#TODO Structured Data\n",
    "#! -----------------------------------------------------------------------------------------\n",
    "# Fetching demographic_no of total patients and OA patients from the DxResearch table\n",
    "df = pd.read_csv(\"/home/jovyan/mpc_use_case/prototype/oaTypes/DxResearch.csv\")\n",
    "# df.head()\n",
    "for index, row in df.iterrows():\n",
    "    no = row['demographic_no']\n",
    "    all_demographic_nos_dxresearch.add(no)\n",
    "    if row['dxresearch_code'] == 715:\n",
    "        oa_patients.add(no)\n",
    "print(\"\\nNumber of patients listed with disease code:\", len(all_demographic_nos_dxresearch))\n",
    "print(\"Number of patients listed in disease code table as having OA:\", len(oa_patients))\n",
    "\n",
    "#! -----------------------------------------------------------------------------------------\n",
    "#TODO Where PSI comes in\n",
    "#! -----------------------------------------------------------------------------------------\n",
    "# Deducting the demographic_no of OA patients having notes\n",
    "oa_patients_with_notes = oa_patients.intersection(all_demographic_nos_notes)\n",
    "#print(\"Number of patients having OA and notes:\", len(oa_patients_with_notes))\n",
    "print(\"Patient IDs:\", sorted(oa_patients_with_notes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function combines all the notes from all patients and moves the combined notes to another folder.\n",
    "# The notes are separated by whether the patients have OA or not\n",
    "def combine_files(files, oa_patients_with_notes, notes_folder, new_folder_name, flag):\n",
    "    for file in files:\n",
    "        demographic_no = file.split(\"-\")[1].split(\".\")[0]\n",
    "        if flag == 0:\n",
    "            if int(demographic_no) not in oa_patients_with_notes:\n",
    "                with open(os.path.join(notes_folder, file), 'r') as fr:\n",
    "                    text = fr.read()\n",
    "                    fr.close()\n",
    "                with open(new_folder_name + '/' + demographic_no + \".txt\", \"a\") as fw:\n",
    "                    fw.write(text)\n",
    "                    fw.write(\"\\n\")\n",
    "                    fw.close()\n",
    "        else:\n",
    "            if int(demographic_no) in oa_patients_with_notes:\n",
    "                with open(os.path.join(notes_folder, file), 'r') as fr:\n",
    "                    text = fr.read()\n",
    "                    fr.close()\n",
    "                with open(new_folder_name + '/' + demographic_no + \".txt\", \"a\") as fw:\n",
    "                    fw.write(text)\n",
    "                    fw.write(\"\\n\")\n",
    "                    fw.close()\n",
    "                \n",
    "combine_files(files, oa_patients_with_notes, deid_notes_path, other_patients_combined_notes, flag=0)\n",
    "combine_files(files, oa_patients_with_notes, deid_notes_path, oa_patients_combined_notes, flag=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This adds every individual word into a new list, and splits up strings that contain more than one word so that their\n",
    "# individual words can be added to the list. The new list contains unique individual words that are found in any of the original\n",
    "# strings. Returns a dictionary of individual words and their associated embeddings\n",
    "def create_embeddings(substrings):  \n",
    "    vectors = {}\n",
    "    single_words =[]\n",
    "    for i in substrings:\n",
    "        if len(i.split()) == 1:\n",
    "            single_words.append(i)\n",
    "        else:\n",
    "            two_words = i.split()\n",
    "            for word in two_words:\n",
    "                if word not in single_words:\n",
    "                    single_words.append(word)\n",
    "    for word in single_words:\n",
    "        # print(word)\n",
    "        try:\n",
    "            word_array = bioword_vector[word]\n",
    "        # If the word does not have an embedding already, we use the model to create one for it\n",
    "        except:\n",
    "            word_array = bioword_model.get_word_vector(word)\n",
    "        vectors[word] = word_array\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to preprocess a clinical note. It removes any undesired symbols and stopwords, so that only words remain in the note\n",
    "def preprocess(txt):\n",
    "    BAD_SYMBOLS_RE = re.compile('[0-9a-z #+_]')\n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    p = re.compile(\"[\" + re.escape(string.punctuation) + \"]\")\n",
    "    txt = txt.lower()\n",
    "    #txt = BAD_SYMBOLS_RE.sub('', txt)\n",
    "    txt = txt.translate(remove_digits)\n",
    "    txt = p.sub(\"\", txt)\n",
    "    txt = unidecode.unidecode(txt)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(txt)\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            # stem_w = ps.stem(w)\n",
    "            filtered_sentence.append(w)\n",
    "    return \" \".join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the word embeddings into tensors\n",
    "def create_tensors(embeddings):\n",
    "    tensors = []\n",
    "    for key in embeddings:\n",
    "        tensor = torch.Tensor(embeddings[key])\n",
    "        tensors.append(tensor)\n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This function creates pickle files that can be retrieved later in the MPC protocol\n",
    "# def create_file(filename, tensors):\n",
    "#     with open(filename, 'wb') as f:\n",
    "#         file = pickle.dump(tensors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates pickle files that can be retrieved later in the MPC protocol\n",
    "def create_file(filename, tensors):\n",
    "    with open(filename, 'wb') as f:\n",
    "        torch.save(tensors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_718/2146089897.py:5: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642881969/work/torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  tensor = torch.Tensor(embeddings[key])\n"
     ]
    }
   ],
   "source": [
    "# For each patient, taking all the combined notes, extracting the words, then creating embeddings for each word\n",
    "# After the embeddings are created, we can create CrypTen tensors and stored the tensors into a file for later use\n",
    "oa_files = natsorted(os.listdir(oa_patients_combined_notes))\n",
    "all_tensors = []\n",
    "notes_embeddings = []\n",
    "\n",
    "# For each patient in the folder\n",
    "for notes_file in oa_files:\n",
    "    oa_demographic_no = int(notes_file.split(\".\")[0])\n",
    "    # print(oa_demographic_no)\n",
    "    with open(os.path.join(oa_patients_combined_notes, notes_file), 'r') as fr:\n",
    "        note_data = fr.read()\n",
    "        fr.close()\n",
    "    # Preprocess the note\n",
    "    preprocessed_note = preprocess(note_data)\n",
    "    # Split the note into alist of individual words\n",
    "    list_preprocessed_note = preprocessed_note.split()\n",
    "    # Create an embedding for each word\n",
    "    embeddings = create_embeddings(list_preprocessed_note)\n",
    "    # notes_embeddings.append(embeddings)\n",
    "    note_tensors = create_tensors(embeddings)\n",
    "    # all_tensors.append(note_tensors)\n",
    "    # Save the tensors to a file    ** Might have to do this in the protocol itself\n",
    "    create_file(os.path.join(oa_patients_tensors, str(oa_demographic_no) + \".pt\"), note_tensors)\n",
    "    # create_file(os.path.join(oa_patients_tensors, str(oa_demographic_no) + \".pkl\"), note_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #! This creates encrypted tensors (in a .pth file)!!!!!!!!!\n",
    "\n",
    "# # Convert the word embeddings into tensors\n",
    "# def create_tensors(embeddings):\n",
    "#     tensors = []\n",
    "#     for key in embeddings:\n",
    "#         tensor = torch.Tensor(embeddings[key])\n",
    "#         encrypted_tensor = crypten.cryptensor(tensor)\n",
    "#         tensors.append(encrypted_tensor)\n",
    "#     return tensors\n",
    "\n",
    "# # For each patient, taking all the combined notes, extracting the words, then creating embeddings for each word\n",
    "# # After the embeddings are created, we can create CrypTen tensors and stored the tensors into a file for later use\n",
    "# oa_files = natsorted(os.listdir(oa_patients_combined_notes))\n",
    "# all_tensors = []\n",
    "# notes_embeddings = []\n",
    "\n",
    "# # For each patient in the folder\n",
    "# for notes_file in oa_files:\n",
    "#     oa_demographic_no = int(notes_file.split(\".\")[0])\n",
    "#     # print(oa_demographic_no)\n",
    "#     with open(os.path.join(oa_patients_combined_notes, notes_file), 'r') as fr:\n",
    "#         note_data = fr.read()\n",
    "#         fr.close()\n",
    "#     # Preprocess the note\n",
    "#     preprocessed_note = preprocess(note_data)\n",
    "#     # Split the note into alist of individual words\n",
    "#     list_preprocessed_note = preprocessed_note.split()\n",
    "#     # Create an embedding for each word\n",
    "#     embeddings = create_embeddings(list_preprocessed_note)\n",
    "#     notes_embeddings.append(embeddings)\n",
    "#     # Create a CrypTen tensor for each embedding\n",
    "#     note_tensors = create_tensors(embeddings)\n",
    "#     all_tensors.append(note_tensors)\n",
    "#     # Save the tensors to a file    ** Might have to do this in the protocol itself\n",
    "#     crypten.save(note_tensors, os.path.join(oa_patients_encrypted_tensors, str(oa_demographic_no) + \".pth\"))\n",
    "#     # create_file(os.path.join(oa_patients_tensors, str(oa_demographic_no) + \".pkl\"), note_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_embeddings = []\n",
    "# test_embedding = torch.Tensor(bioword_model.get_word_vector(\"osteoarthritis\"))\n",
    "# list_embeddings.append(test_embedding)\n",
    "# file = \"/home/jovyan/mpc_use_case/three_party_mpc/party1_unstructured/test_embedding.pt\"\n",
    "# torch.save(list_embeddings, file)\n",
    "# print(len(list_embeddings))\n",
    "# test = crypten.load(file)\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = torch.load(\"/home/jovyan/mpc_use_case/three_party_mpc/party1_unstructured/data/oa_patients_tensors/4.pt\")\n",
    "# test2 = torch.load(\"/home/jovyan/mpc_use_case/three_party_mpc/party2_structured/data/convert_data_output/hip_keywords.pt\")\n",
    "# print(len(test))\n",
    "# print(len(test2))\n",
    "\n",
    "# print(test[0])\n",
    "# print(test2[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21cdf2ca7b6a1c580853a0bc0053a76de6c90b3ccd0190caebfcae48bebcb99a"
  },
  "kernelspec": {
   "display_name": "Python [conda env:mpyc]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
