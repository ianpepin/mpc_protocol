{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import fasttext\n",
    "import sys\n",
    "import torch\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import unidecode\n",
    "from natsort import natsorted\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths of different files used in this script\n",
    "vectors_filename = \"/home/jovyan/embeddings/BioWordVec_PubMed_MIMICIII_d200.vec.bin\"\n",
    "model_filename = \"/home/jovyan/embeddings/BioWordVec_PubMed_MIMICIII_d200.bin\"\n",
    "\n",
    "deid_notes_path = \"/home/jovyan/mpc_use_case/crypten_unstructured_data/data/deidentified_notes\"\n",
    "oa_patients_combined_notes = \"/home/jovyan/mpc_use_case/crypten_unstructured_data/data/oa_patients_notes/oa_patients\"\n",
    "other_patients_combined_notes = \"/home/jovyan/mpc_use_case/crypten_unstructured_data/data/other_patients_notes\"\n",
    "\n",
    "oa_patients_tensors = \"/home/jovyan/mpc_use_case/crypten_unstructured_data/data/oa_patients_tensors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors loaded\n"
     ]
    }
   ],
   "source": [
    "# Loads the word embeddings from the vector binary file\n",
    "bioword_vector = KeyedVectors.load_word2vec_format(vectors_filename, binary=True)\n",
    "print(\"Vectors loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# Loads the BioWordVec model, which we can use to generate embeddings for OOV words\n",
    "bioword_model = fasttext.load_model(model_filename)\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing patients and their notes based on their diagnosis\n",
      "Number of patients having patient notes: 163\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nProcessing patients and their notes based on their diagnosis\")\n",
    "\n",
    "# Fetching demographic_no for all patients from the filename of notes\n",
    "files = natsorted(os.listdir(deid_notes_path))\n",
    "all_demographic_nos_notes = set()\n",
    "# This is a list that contains the note IDs for each patient\n",
    "for file in files:\n",
    "    # print(file)\n",
    "    demographic_no = int(file.split(\"-\")[1].split(\".\")[0])\n",
    "    all_demographic_nos_notes.add(demographic_no)\n",
    "    \n",
    "list_of_files = [[] for _ in range(len(all_demographic_nos_notes))]\n",
    "for file in files:\n",
    "    demographic_no = int(file.split(\"-\")[1].split(\".\")[0])\n",
    "    note_id = int(file.split(\"-\")[0].split(\".\")[0])\n",
    "    # Add the note ID to the list of notes for the patient\n",
    "    list_of_files[demographic_no-1].append(note_id)\n",
    "print(\"Number of patients having patient notes:\", len(all_demographic_nos_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of patients listed with disease code: 163\n",
      "Number of patients listed in disease code table as having OA: 34\n",
      "Patient IDs: [4, 5, 6, 7, 8, 9, 11, 14, 18, 26, 37, 40, 54, 58, 61, 63, 64, 75, 77, 83, 94, 101, 103, 106, 110, 115, 133, 135, 148, 150, 153, 155, 159, 162]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#! -----------------------------------------------------------------------------------------\n",
    "#TODO Structured data\n",
    "#! -----------------------------------------------------------------------------------------\n",
    "all_demographic_nos_dxresearch = set()\n",
    "oa_patients = set()\n",
    "# Convert txt to csv\n",
    "with open('/home/jovyan/mpc_use_case/structured_data/DxResearch.txt', 'r') as in_file:\n",
    "    stripped = (line.strip() for line in in_file)\n",
    "    lines = (line.split(\",\") for line in stripped if line)\n",
    "    with open('/home/jovyan/mpc_use_case/structured_data/DxResearch.csv', 'w') as out_file:\n",
    "        writer = csv.writer(out_file)\n",
    "        writer.writerows(lines)\n",
    "\n",
    "#! -----------------------------------------------------------------------------------------\n",
    "#TODO Structured Data\n",
    "#! -----------------------------------------------------------------------------------------\n",
    "# Fetching demographic_no of total patients and OA patients from the DxResearch table\n",
    "df = pd.read_csv(\"/home/jovyan/mpc_use_case/structured_data/DxResearch.csv\")\n",
    "# df.head()\n",
    "for index, row in df.iterrows():\n",
    "    no = row['demographic_no']\n",
    "    all_demographic_nos_dxresearch.add(no)\n",
    "    if row['dxresearch_code'] == 715:\n",
    "        oa_patients.add(no)\n",
    "print(\"\\nNumber of patients listed with disease code:\", len(all_demographic_nos_dxresearch))\n",
    "print(\"Number of patients listed in disease code table as having OA:\", len(oa_patients))\n",
    "\n",
    "#! -----------------------------------------------------------------------------------------\n",
    "#TODO Where PSI comes in\n",
    "#! -----------------------------------------------------------------------------------------\n",
    "# Deducting the demographic_no of OA patients having notes\n",
    "oa_patients_with_notes = oa_patients.intersection(all_demographic_nos_notes)\n",
    "#print(\"Number of patients having OA and notes:\", len(oa_patients_with_notes))\n",
    "print(\"Patient IDs:\", sorted(oa_patients_with_notes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function combines all the notes from all patients and moves the combined notes to another folder.\n",
    "# The notes are separated by whether the patients have OA or not\n",
    "def combine_files(files, oa_patients_with_notes, notes_folder, new_folder_name, flag):\n",
    "    for file in files:\n",
    "        demographic_no = file.split(\"-\")[1].split(\".\")[0]\n",
    "        # for id_number in all_demographic_nos_notes:\n",
    "        #     if id_number == int(demographic_no):\n",
    "        if flag == 0:\n",
    "            if int(demographic_no) not in oa_patients_with_notes:\n",
    "                with open(os.path.join(notes_folder, file), 'r') as fr:\n",
    "                    text = fr.read()\n",
    "                    fr.close()\n",
    "                if not os.path.exists(new_folder_name):\n",
    "                    os.makedirs(new_folder_name)\n",
    "                with open(new_folder_name + '/' + demographic_no + \".txt\", \"a\") as fw:\n",
    "                    fw.write(text)\n",
    "                    fw.write(\"\\n\")\n",
    "                    fw.close()\n",
    "        else:\n",
    "            if int(demographic_no) in oa_patients_with_notes:\n",
    "                with open(os.path.join(notes_folder, file), 'r') as fr:\n",
    "                    text = fr.read()\n",
    "                    fr.close()\n",
    "                if not os.path.exists(new_folder_name):\n",
    "                    os.makedirs(new_folder_name)\n",
    "                with open(new_folder_name + '/' + demographic_no + \".txt\", \"a\") as fw:\n",
    "                    fw.write(text)\n",
    "                    fw.write(\"\\n\")\n",
    "                    fw.close()\n",
    "                        \n",
    "# combine_files(files, oa_patients_with_notes, deid_notes_path, other_patients_combined_notes, flag=0)\n",
    "# combine_files(files, oa_patients_with_notes, deid_notes_path, oa_patients_combined_notes, flag=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This adds every individual word into a new list, and splits up strings that contain more than one word so that their\n",
    "# individual words can be added to the list. The new list contains unique individual words that are found in any of the original\n",
    "# strings. Returns a dictionary of individual words and their associated embeddings\n",
    "def create_embeddings(substrings):  \n",
    "    vectors = []\n",
    "    for word in substrings:\n",
    "        # print(word)\n",
    "        try:\n",
    "            word_array = bioword_vector[word]\n",
    "        # If the word does not have an embedding already, we use the model to create one for it\n",
    "        except:\n",
    "            word_array = bioword_model.get_word_vector(word)\n",
    "        vectors.append(word_array)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to preprocess a clinical note. It removes any undesired symbols and stopwords, so that only words remain in the note\n",
    "def preprocess(txt):\n",
    "    BAD_SYMBOLS_RE = re.compile('[0-9a-z #+_]')\n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    p = re.compile(\"[\" + re.escape(string.punctuation) + \"]\")\n",
    "    txt = txt.lower()\n",
    "    #txt = BAD_SYMBOLS_RE.sub('', txt)\n",
    "    txt = txt.translate(remove_digits)\n",
    "    txt = p.sub(\"\", txt)\n",
    "    txt = unidecode.unidecode(txt)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(txt)\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            # stem_w = ps.stem(w)\n",
    "            filtered_sentence.append(w)\n",
    "    # return \" \".join(filtered_sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the word embeddings into tensors\n",
    "def create_tensors(embeddings):\n",
    "    tensors = []\n",
    "    for embedding in embeddings:\n",
    "        tensor = torch.Tensor(embedding)\n",
    "        tensors.append(tensor)\n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates pickle files that can be retrieved later in the MPC protocol\n",
    "def create_file(filename, tensors):\n",
    "    with open(filename, 'wb') as f:\n",
    "        torch.save(tensors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each patient, taking all the combined notes, extracting the words, then creating embeddings for each word\n",
    "# After the embeddings are created, we can create CrypTen tensors and stored the tensors into a file for later use\n",
    "oa_files = natsorted(os.listdir(oa_patients_combined_notes))\n",
    "all_tensors = []\n",
    "notes_embeddings = []\n",
    "\n",
    "# For each patient in the folder\n",
    "for notes_file in oa_files:\n",
    "    oa_demographic_no = int(notes_file.split(\".\")[0])\n",
    "    # print(oa_demographic_no)\n",
    "    with open(os.path.join(oa_patients_combined_notes, notes_file), 'r') as fr:\n",
    "        note_data = fr.read()\n",
    "        fr.close()\n",
    "    # Preprocess the note\n",
    "    preprocessed_note = preprocess(note_data)\n",
    "    # Create an embedding for each word\n",
    "    embeddings = create_embeddings(preprocessed_note)\n",
    "    # notes_embeddings.append(embeddings)\n",
    "    note_tensors = create_tensors(embeddings)\n",
    "    # all_tensors.append(note_tensors)\n",
    "    # Save the tensors to a file    ** Might have to do this in the protocol itself\n",
    "    create_file(os.path.join(oa_patients_tensors, str(oa_demographic_no) + \".pt\"), note_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.cuda.is_available())\n",
    "\n",
    "# print(torch.cuda.device_count())\n",
    "\n",
    "# print(torch.cuda.current_device())\n",
    "\n",
    "# print(torch.cuda.device(0))\n",
    "\n",
    "# print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"/home/jovyan/mpc_use_case/crypten_unstructured_data/data/oa_patients_notes/notes_all_patients\"\n",
    "test = os.listdir(dir_name)\n",
    "\n",
    "for item in test:\n",
    "    if item.endswith(\".pt\"):\n",
    "        os.remove(os.path.join(dir_name, item))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21cdf2ca7b6a1c580853a0bc0053a76de6c90b3ccd0190caebfcae48bebcb99a"
  },
  "kernelspec": {
   "display_name": "Python [conda env:mpyc]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
