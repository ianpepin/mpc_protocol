{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "# import codecs\n",
    "import gensim\n",
    "# import time\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import fasttext\n",
    "import sys\n",
    "import torch\n",
    "import crypten\n",
    "crypten.init()\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_filename = \"/home/jovyan/embeddings/BioWordVec_PubMed_MIMICIII_d200.vec.bin\"\n",
    "model_filename = \"/home/jovyan/embeddings/BioWordVec_PubMed_MIMICIII_d200.bin\"\n",
    "\n",
    "#! Remove this path once I implement PSI\n",
    "deid_notes_path = \"/home/jovyan/mpc_use_case/three_party_mpc/party1_unstructured/data/deidentified_notes\"\n",
    "\n",
    "knee_tensors_path = \"/home/jovyan/mpc_use_case/three_party_mpc/party2_structured/data/single_word_tensors/knee_oa_tensors.pth\"\n",
    "hip_tensors_path = \"/home/jovyan/mpc_use_case/three_party_mpc/party2_structured/data/single_word_tensors/hip_oa_tensors.pth\"\n",
    "\n",
    "knee_keywords_output = \"/home/jovyan/mpc_use_case/three_party_mpc/party2_structured/data/convert_data_output/knee_keywords_test.pt\"\n",
    "hip_keywords_output = \"/home/jovyan/mpc_use_case/three_party_mpc/party2_structured/data/convert_data_output/hip_keywords_test.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors loaded\n"
     ]
    }
   ],
   "source": [
    "bioword_vector = KeyedVectors.load_word2vec_format(vectors_filename, binary=True)\n",
    "print(\"Vectors loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "bioword_model = fasttext.load_model(model_filename)\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing patients and their notes based on their diagnosis\n",
      "Number of patients having patient notes: 163\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#! -----------------------------------------------------------------------------------------\n",
    "#TODO Untructured data - Remove after\n",
    "#! -----------------------------------------------------------------------------------------\n",
    "print(\"\\nProcessing patients and their notes based on their diagnosis\")\n",
    "\n",
    "# Fetching demographic_no for all patients from the filename of notes\n",
    "files = os.listdir(deid_notes_path)\n",
    "all_demographic_nos_notes = set()\n",
    "oa_patients = set()\n",
    "for file in files:\n",
    "    demographic_no = int(file.split(\"-\")[1].split(\".\")[0])\n",
    "    all_demographic_nos_notes.add(demographic_no)\n",
    "print(\"Number of patients having patient notes:\", len(all_demographic_nos_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of patients listed with disease code: 163\n",
      "Number of patients listed in disease code table as having OA: 33\n"
     ]
    }
   ],
   "source": [
    "all_demographic_nos_dxresearch = set()\n",
    "oa_patients = set()\n",
    "\n",
    "# Convert txt to csv\n",
    "with open('/home/jovyan/mpc_use_case/structured_data/DxResearch.txt', 'r') as in_file:\n",
    "    stripped = (line.strip() for line in in_file)\n",
    "    lines = (line.split(\",\") for line in stripped if line)\n",
    "    with open('/home/jovyan/mpc_use_case/prototype/oaTypes/DxResearch.csv', 'w') as out_file:\n",
    "        writer = csv.writer(out_file)\n",
    "        writer.writerows(lines)\n",
    "\n",
    "# Fetching demographic_no of total patients and OA patients from the DxResearch table\n",
    "df = pd.read_csv(\"/home/jovyan/mpc_use_case/prototype/oaTypes/DxResearch.csv\")\n",
    "# df.head()\n",
    "for index, row in df.iterrows():\n",
    "    no = row['demographic_no']\n",
    "    all_demographic_nos_dxresearch.add(no)\n",
    "    if row['dxresearch_code'] == 715:\n",
    "        oa_patients.add(no)\n",
    "print(\"\\nNumber of patients listed with disease code:\", len(all_demographic_nos_dxresearch))\n",
    "print(\"Number of patients listed in disease code table as having OA:\", len(oa_patients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient IDs: [4, 5, 6, 7, 8, 9, 11, 14, 18, 26, 37, 40, 54, 58, 61, 63, 64, 76, 77, 83, 94, 101, 103, 106, 110, 115, 133, 135, 148, 150, 155, 159, 162]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#! -----------------------------------------------------------------------------------------\n",
    "#TODO Where PSI comes in\n",
    "#! -----------------------------------------------------------------------------------------\n",
    "# Deducting the demographic_no of OA patients having notes\n",
    "oa_patients_with_notes = oa_patients.intersection(all_demographic_nos_notes)\n",
    "#print(\"Number of patients having OA and notes:\", len(oa_patients_with_notes))\n",
    "print(\"Patient IDs:\", sorted(oa_patients_with_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists that contain keywords that we can look for in the clinical notes\n",
    "knee_oa_substrings = ['knee pain', 'pain knee', 'knee oa', 'oa knee', 'osteoarthrities knee', 'knee osteoarthritis', \n",
    "                      'kneepain', 'painknee', 'kneeoa', 'oaknee', 'osteoarthritiesknee', 'kneeosteoarthritis']\n",
    "hip_oa_substrings = ['hip pain', 'pain hip', 'hip oa', 'oa hip', 'osteoarthrities hip', 'hip osteoarthritis',\n",
    "                     'hippain', 'painhip', 'hipoa', 'oahip', 'osteoarthritieship', 'hiposteoarthritis']\n",
    "\n",
    "# knee_oa_substrings = ['kneepain', 'painknee', 'kneeoa', 'oaknee', 'osteoarthritiesknee', 'kneeosteoarthritis']\n",
    "# hip_oa_substrings = ['hip pain', 'pain hip', 'hip oa', 'oa hip', 'osteoarthrities hip', 'hip osteoarthritis',\n",
    "#                      'hippain', 'painhip', 'hipoa', 'oahip', 'osteoarthritieship', 'hiposteoarthritis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['kneepain', 'painknee', 'kneeoa', 'oaknee', 'osteoarthritiesknee', 'kneeosteoarthritis'])\n",
      "dict_keys(['hip', 'pain', 'oa', 'osteoarthrities', 'osteoarthritis', 'hippain', 'painhip', 'hipoa', 'oahip', 'osteoarthritieship', 'hiposteoarthritis'])\n"
     ]
    }
   ],
   "source": [
    "# This adds every individual word into a new list, and splits up strings that contain more than one word so that their\n",
    "# individual words can be added to the list. The new list contains unique individual words that are found in any of the original\n",
    "# strings. Returns a dictionary of individual words and their associated embeddings\n",
    "def create_embeddings(substrings):  \n",
    "    vectors = {}\n",
    "    single_words =[]\n",
    "    for i in substrings:\n",
    "        if len(i.split()) == 1:\n",
    "            single_words.append(i)\n",
    "        else:\n",
    "            two_words = i.split()\n",
    "            for word in two_words:\n",
    "                if word not in single_words:\n",
    "                    single_words.append(word)\n",
    "    for word in single_words:\n",
    "        # print(word)\n",
    "        try:\n",
    "            word_array = bioword_vector[word]\n",
    "        # If the word does not have an embedding already, we use the model to create one for it\n",
    "        except:\n",
    "            word_array = bioword_model.get_word_vector(word)\n",
    "        vectors[word] = word_array\n",
    "    return vectors\n",
    "\n",
    "knee_oa_embeddings = create_embeddings(knee_oa_substrings)\n",
    "hip_oa_embeddings = create_embeddings(hip_oa_substrings)\n",
    "\n",
    "print(knee_oa_embeddings.keys())\n",
    "print(hip_oa_embeddings.keys())\n",
    "    \n",
    "# print(np.sqrt(np.sum(np.square(knee_oa_embeddings['oa'] - hip_oa_embeddings['oa']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the word embeddings into CrypTen tensors, save the encrypted tensors to a file for future use\n",
    "# def create_tensors(list_embeddings):\n",
    "#     tensors = []\n",
    "#     for key in list_embeddings:\n",
    "#         # print(list_embeddings[key])\n",
    "#         tensor = torch.Tensor(list_embeddings[key])\n",
    "#         encrypted_tensor = crypten.cryptensor(tensor)\n",
    "#         tensors.append(encrypted_tensor)\n",
    "#     return tensors\n",
    "\n",
    "# knee_oa_tensors = create_tensors(knee_oa_embeddings)\n",
    "# hip_oa_tensors = create_tensors(hip_oa_embeddings)\n",
    "\n",
    "# # print(knee_oa_tensors[0].get_plain_text())\n",
    "# crypten.save(knee_oa_tensors, knee_tensors_path)\n",
    "# crypten.save(hip_oa_tensors, hip_tensors_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_347/2350900036.py:6: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642881969/work/torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  tensor = torch.Tensor(list_embeddings[key])\n"
     ]
    }
   ],
   "source": [
    "# Convert the word embeddings into CrypTen tensors, save the encrypted tensors to a file for future use\n",
    "def create_tensors(list_embeddings):\n",
    "    tensors = []\n",
    "    for key in list_embeddings:\n",
    "        # print(list_embeddings[key])\n",
    "        tensor = torch.Tensor(list_embeddings[key])\n",
    "        # encrypted_tensor = crypten.cryptensor(tensor)\n",
    "        # tensors.append(encrypted_tensor)\n",
    "        tensors.append(tensor)\n",
    "    return tensors\n",
    "\n",
    "knee_oa_tensors = create_tensors(knee_oa_embeddings)\n",
    "hip_oa_tensors = create_tensors(hip_oa_embeddings)\n",
    "\n",
    "\n",
    "# print(knee_oa_tensors[0].get_plain_text())\n",
    "# crypten.save(knee_oa_tensors, knee_tensors_path)\n",
    "# crypten.save(hip_oa_tensors, hip_tensors_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates pickle files that can be retrieved later in the MPC protocol\n",
    "def create_file(filename, tensors):\n",
    "    with open(filename, 'wb') as f:\n",
    "        torch.save(tensors, f)\n",
    "    \n",
    "create_file(knee_keywords_output, knee_oa_tensors)\n",
    "create_file(hip_keywords_output, hip_oa_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_embeddings = []\n",
    "# test_embedding = torch.Tensor(bioword_vector[\"osteoarthritis\"])\n",
    "# test_embedding2 = torch.Tensor(bioword_vector[\"osteoarthrities\"])\n",
    "# list_embeddings.append(test_embedding)\n",
    "# list_embeddings.append(test_embedding2)\n",
    "# # print(test_embedding)\n",
    "# file_encrypted = \"/home/jovyan/mpc_use_case/three_party_mpc/party2_structured/list_embedding.pt\"\n",
    "# torch.save(list_embeddings, file_encrypted)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "396fe45e3b44ba9654315dc2a7f72c0c783ed34682a9bc3ce59f4b8b89d2d6b4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.15 64-bit ('thesis': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
